{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d13ad5-f188-4b49-be18-e590ad332901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded args: Namespace(net='sam', arch='vit_b', baseline='unet', dataset_name='MRI-Prostate', img_folder='./datasets', mask_folder='./datasets', train_img_list='./datasets/MRI-Prostate/train_5shot.csv', val_img_list='./datasets/MRI-Prostate/val_5shot.csv', targets='combine_all', finetune_type='adapter', normalize_type='sam', dir_checkpoint='2D-SAM_vit_b_decoder_adapter_MRI-Prostate_noprompt', num_cls=2, epochs=200, sam_ckpt='sam_vit_b_01ec64.pth', type='map', vis='', reverse=False, pretrain=False, val_freq=100, gpu=True, gpu_device=0, sim_gpu=0, epoch_ini=1, image_size=1024, out_size=256, patch_size=2, dim=512, depth=64, heads=16, mlp_dim=1024, w=4, b=4, s=True, if_warmup=True, warmup_period=200, lr=0.001, uinch=1, imp_lr=0.0003, weights=0, base_weights=0, sim_weights=0, distributed='none', dataset='isic', thd=False, chunk=96, num_sample=4, roi_size=96, if_update_encoder=False, if_encoder_adapter=False, encoder_adapter_depths=[0, 1, 10, 11], if_mask_decoder_adapter=True, decoder_adapt_depth=2, if_encoder_lora_layer=False, if_decoder_lora_layer=False, encoder_lora_layer=[0, 1, 10, 11], if_split_encoder_gpus=False, devices=[0, 1], gpu_fractions=[0.5, 0.5], evl_chunk='')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from argparse import Namespace\n",
    "from models.sam import sam_model_registry\n",
    "\n",
    "arch = \"vit_b\"\n",
    "finetune_type = \"adapter\"\n",
    "dataset_name = \"MRI-Prostate\"\n",
    "\n",
    "# Construct the checkpoint directory argument\n",
    "checkpoint_dir = f\"2D-SAM_{arch}_decoder_{finetune_type}_{dataset_name}_noprompt\"\n",
    "args_path = f\"{checkpoint_dir}/args.json\"\n",
    "\n",
    "# Check if args.json exists\n",
    "if not os.path.exists(args_path):\n",
    "    raise FileNotFoundError(f\"args.json not found at {args_path}\")\n",
    "\n",
    "# Read args.json\n",
    "with open(args_path, \"r\") as f:\n",
    "    args_dict = json.load(f)\n",
    "\n",
    "# Ensure no None values\n",
    "args_dict = {k: (v if v is not None else \"\") for k, v in args_dict.items()}\n",
    "args = Namespace(**args_dict)\n",
    "\n",
    "# Debugging: print loaded arguments\n",
    "print(\"Loaded args:\", args)\n",
    "\n",
    "# Check available architectures\n",
    "print(f\"Available architectures in registry: {list(sam_model_registry.keys())}\")\n",
    "print(f\"Requested architecture: {args.arch}\")\n",
    "\n",
    "try:\n",
    "    # Check if requested architecture is in sam_model_registry\n",
    "    if args.arch not in sam_model_registry:\n",
    "        raise KeyError(f\"Invalid architecture '{args.arch}'. Available: {list(sam_model_registry.keys())}\")\n",
    "\n",
    "    # Load the model\n",
    "    print(f\"Loading model for architecture: {args.arch}\")\n",
    "    sam_fine_tune = sam_model_registry[args.arch](\n",
    "        args, checkpoint=os.path.join(args.dir_checkpoint, 'checkpoint_best.pth'), num_classes=args.num_cls\n",
    "    )\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if torch.cuda.is_available():\n",
    "        sam_fine_tune = sam_fine_tune.to('cuda').eval()\n",
    "    else:\n",
    "        print(\"Warning: CUDA is not available, running on CPU.\")\n",
    "        sam_fine_tune = sam_fine_tune.to('cpu').eval()\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f35640-a2e5-4f0d-a59d-0fa891062cf7",
   "metadata": {},
   "source": [
    "## evaluate a image volume and save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1654874-2247-40c0-81f1-1e19dc400c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a single image slice\n",
    "def evaluate_1_slice(image_path, model):\n",
    "    \"\"\"\n",
    "    Evaluates a single image slice using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path: Path to the image slice file.\n",
    "    - model: The model used for evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - ori_img: The original image after normalization.\n",
    "    - pred: The prediction from the model.\n",
    "    - Pil_img: The PIL image of the original slice.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    Pil_img = img.copy()\n",
    "    \n",
    "    # Resize the image to 1024x1024\n",
    "    img = transforms.Resize((1024, 1024))(img)\n",
    "    \n",
    "    # Transform the image to a tensor and normalize\n",
    "    transform_img = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    img = transform_img(img)\n",
    "    imgs = torch.unsqueeze(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img), 0).cuda()\n",
    "    \n",
    "    # Perform model inference without gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Get image embeddings from the image encoder\n",
    "        img_emb = model.image_encoder(imgs)\n",
    "        \n",
    "        # Get sparse and dense embeddings from the prompt encoder\n",
    "        sparse_emb, dense_emb = model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "        )\n",
    "        \n",
    "        # Get the prediction from the mask decoder\n",
    "        pred, _ = model.mask_decoder(\n",
    "            image_embeddings=img_emb,\n",
    "            image_pe=model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_emb,\n",
    "            dense_prompt_embeddings=dense_emb,\n",
    "            multimask_output=True,\n",
    "        )\n",
    "        \n",
    "        # Get the most likely prediction\n",
    "        pred = pred.argmax(dim=1)\n",
    "    \n",
    "    # Get the original image after normalization\n",
    "    ori_img = inverse_normalize(imgs.cpu()[0])\n",
    "    \n",
    "    return ori_img, pred, Pil_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bf140-fd2c-48aa-8a84-0a5a1dd278af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "image_path = 'path_to_your_image_slice.png'  # Replace with the path to your image slice\n",
    "output_dir = 'output_predictions'  # Directory to save predictions\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Evaluate the image slice\n",
    "ori_img, pred_1, Pil_img1 = evaluate_1_slice(image_path, model)\n",
    "\n",
    "# Convert the predicted mask to a PIL image\n",
    "mask_pred_1 = ((pred_1).cpu()).float()\n",
    "pil_mask1 = Image.fromarray(np.array(mask_pred_1[0], dtype=np.uint8), 'L').resize(Pil_img1.size, resample=Image.NEAREST)\n",
    "\n",
    "# Save the original image and the predicted mask as PNG\n",
    "ori_img_filename = os.path.join(output_dir, 'original_image.png')\n",
    "mask_img_filename = os.path.join(output_dir, 'predicted_mask.png')\n",
    "\n",
    "Pil_img1.save(ori_img_filename)\n",
    "pil_mask1.save(mask_img_filename)\n",
    "\n",
    "print(f\"Original image saved to {ori_img_filename}\")\n",
    "print(f\"Predicted mask saved to {mask_img_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d415e43d-012b-45bf-91c2-55f671830b6d",
   "metadata": {},
   "source": [
    "## visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9105aa1-da83-4b26-81f0-a445240cea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the original image and the predicted mask\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Display the original slice\n",
    "ori_img_display = np.array(Pil_img1)\n",
    "axes[0].imshow(ori_img_display)\n",
    "axes[0].set_title(\"Original Slice\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Display the predicted mask\n",
    "mask_display = np.array(pil_mask1)\n",
    "axes[1].imshow(mask_display, cmap='gray')\n",
    "axes[1].set_title(\"Predicted Mask\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb1056-fe48-4569-a3dc-7dce67265a59",
   "metadata": {},
   "source": [
    "## visualize results as well as ground truth if ground truth path is provided as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c4313a-6955-4233-a004-963dc74c931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path = 'path_to_your_ground_truth.png'  # Replace with the path to your ground truth mask if available\n",
    "# Visualize the results\n",
    "fig, axes = plt.subplots(1, 3 if ground_truth_path else 2, figsize=(18, 6) if ground_truth_path else (12, 6))\n",
    "\n",
    "# Display the original slice\n",
    "ori_img_display = np.array(Pil_img1)\n",
    "axes[0].imshow(ori_img_display)\n",
    "axes[0].set_title(\"Original Slice\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Display the predicted mask\n",
    "mask_display = np.array(pil_mask1)\n",
    "axes[1].imshow(mask_display, cmap='gray')\n",
    "axes[1].set_title(\"Predicted Mask\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Display the ground truth mask if provided\n",
    "if ground_truth_path:\n",
    "    ground_truth_img = Image.open(ground_truth_path).convert('L').resize(Pil_img1.size, resample=Image.NEAREST)\n",
    "    ground_truth_display = np.array(ground_truth_img)\n",
    "    axes[2].imshow(ground_truth_display, cmap='gray')\n",
    "    axes[2].set_title(\"Ground Truth Mask\")\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    # Compute and print the Dice Similarity Coefficient for each class\n",
    "    num_classes = 2  # Replace with the actual number of classes\n",
    "    cls_dsc = [0] * num_classes\n",
    "    for cls in range(num_classes):\n",
    "        mask_pred_cls = (mask_pred_1 == cls).float()\n",
    "        mask_gt_cls = (ground_truth_display == cls).float()\n",
    "        cls_dsc[cls] = dice_coeff(mask_pred_cls, mask_gt_cls).item()\n",
    "        print(f\"Dice Similarity Coefficient for class {cls}: {cls_dsc[cls]}\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hg_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
